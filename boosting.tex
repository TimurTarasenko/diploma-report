\section{Усиление простых классификаторов. Алгоритм AdaBoost}
Усиление простых классификаторов --- подход к решению задачи классификации, распознавания путем комбинирования примитивных ``слабых'' (\emph{weak}) классификаторов в один ``сильный'' (\emph{strong}). Под ``силой'' классификатора в данном случае подразумевается эффективность (качество) решения задачи классификации. В данном разделе речь пойдет о семействе алгоритмов, в основе которых лежит алгоритм \emph{AdaBoost}, описанный в \cite{freund99}. Этот алгоритм был успешно использован во многих областях, в частности для задачи поиска лиц на изображении. Подход усиления простых классификаторов применяется во многих задачах и до сих пор является объектом множества как прикладных, так и теоретических исследований.

\subsection{Введение}
В основе метода усиления простых классификаторов лежит простая предпосылка: скомбинировать некоторое количество элементарных (простых) признаков таким образом, чтобы получить один, но более мощный. Классический пример: пусть человек, играющий на скачках, решил создать программу, которая бы предсказывала, придет ли интересующая его лошадь первой к финишу. Опросив некоторое количество играющих людей, он смог определить несколько эмпирических правил: ставь на лощадь, которая победила в трех предыдущих заездах, ставь на лощадь, ставки на которую максимальны и т.д. Ясно, что каждой из таких правил по отдельности недостаточно надежно и встает вопрос можно ли оптимально скомбинировать для получения надежных результатов.

\subsection{AdaBoost}
Мы рассмотрим один из самых ранних алгоритмов из данного семейства --- AdaBoost. Этот алгоритм был опубликован в 1996 году и послужил основой для всех последующих исследований в данной области. На его основе была построена на данных момент, пожалуй, самая эффективная (как по уровню распознавания, так и по скорости работы) система поиска объектов на изображении \cite{viola01}. На данный момент наиболее распростаненными вариантами базового алгоритма ялвляются \emph{Gentle AdaBoost} и \emph{Real AdaBoost}, превосходящие базовый алгоритм по своим характеристикам, но сохраняющие все основные принципы. К основным достоинствам \emph{AdaBoost} и его вариантов можно отнести высокую скорость работы, высокую эффективность распознавания, простоту реализации, общность.

\subsubsection{Описание алгоритма}
Требуется построить классифицирующую функцию $F : X \to Y$, где $X$ --- пространство векторов признаков, $Y$ --- пространство меток классов. Пусть в нашем распоряжении имеется обучающая выборка $(x_1, y_1), \dots, (x_N, y_n)$, где $x_i \in X$ --- вектор признаков, а $y_i \in Y$ --- метка класса, к которому принадлежит $x_i$. Для простоты изложения мы будем рассматривать задачу с двумя классами, то есть $Y = \lbrace-1; +1\rbrace$. Также у нас есть семейство простых классифицирующих функций $H: X \to Y$. Мы будем строить финальный классификатор в следующей форме:
\begin{displaymath}
  F(x) = \sign{\sum_{m=0}^M{\alpha_mh_m(x)}},
\end{displaymath}
где $\alpha_m \in R, h_m \in H.$ Построим итеративный процесс, где на каждом шаге будем добавлять новое слагаемое:
\begin{displaymath}
  f_m = \alpha_mh_m(x),
\end{displaymath}
вычисляя его с учетом работы уже построенной части классификатора.

На каждом шаге будем для каждого примера $(x_i, y_i)$ из обучающей выборки вычислять его ``вес''. Положим $D_o(i) = \frac{1}{N}$, тогда
\begin{displaymath}
  D_{m + 1}(i) = \frac{D_m(i)\exp{(-y_if_m(x_i))}}{Z_i},
\end{displaymath}
где $Z_i$ --- нормализующий коэффициент, такой что
\begin{displaymath}
  \sum_{i = 1}^N{D_{m + 1}(i)} = 1.
\end{displaymath}

Вес каждого элемента обучающей выборки на текущем шаге задает ``важность'' этого примера для очередного шага обучения алгоритма. Чем больше вес, тем больше алгоритм будет ``стараться'' на данном шаге классифицировать этот пример правильно. Как видно из формулы, чем уверенней пример распознается предыдущими шагами, тем его вес меньше; таким образом, самые большие веса получают примеры, которые предыдущими шагами были классифицированы неверно. Иначе говоря, мы варьируем веса таким образом, чтобы классификатор, включенный в комитет на текущем шаге, ``концентрировался'' на примерах,  с которыми предыдущие шаги ``не справились''. Таким образом на каждом шаге мы работаем с какой-то частью данных, плохо классифицируемой предыдущими шагами, а в итоге комбинируем все промежуточные результаты.

Очередной простой классификатор мы будем выбирать, исходя из взвешенной с распределением $D_m$ ошибки. Мы выбираем (тренируем) $h_m \in H$, минимизирующий взвешенную ошибку классификации
\begin{displaymath}
  e_m = \sum_{i = 1}^N{D_m(i) \cdot (h_m(x_i) \neq y_i)}.
\end{displaymath}

Заметим, что если рассмотреть $D_m$ как распределение вероятности над $X$, что правомерно так как
\begin{gather*}
  \sum_{i = 1}^N{D_{m + 1}(i)} = 1, \text{то}\\
  e_m = \underset{x \sim D_m}{\Pr}(h(x) \neq y).
\end{gather*}

Далее вычисляется вклад текущего слагаемого классифицирующей функции
\begin{displaymath}
  \alpha_m = \log{\frac{1 - e_m}{e_m}}.
\end{displaymath}

Мы продолжаем процесс до некоторого шага $M$, номер которого определяется вручную.

\subsubsection{Роль простого классификатора}
В этом разделе мы уделим внимание фундаменту всех методов усиления простых классификаторов --- семейству простых классификаторов $H : X \to Y$.

Что это такое? Для ясности приведем пример. Пусть входные данные --- это $n$-мерные вектора $X \in R^n$, тогда
\begin{displaymath}
  H = h^{\Theta, k}(x = (x_1, \dots, x_k, \dots, x_n)) = \begin{cases}+1, x_k > \Theta,\\ -1, x_k < \Theta\end{cases},
\end{displaymath}
то есть это порог по $k$-той координате. Такой классификатор в англоязычной литературе носит имя ``пень'' (\emph{stump}) --- основа дерева.

Как при таком множестве $H$ происходит выбор наилучшего классификатора $h^{\Theta, k}$ на каждой итерации? В данном случае делается следующее: для каждого $k = 1..n$ вычисляется порог $\Theta'_k$, реализующий минимум взвешенной ошибки $e_m$, затем из полученных классификаторов $h^{\Theta, k}, k = 1..n$ выбирается соответствующий минимальной $e_m$.

Несмотря на свою простоту, этот классификатор, усиленный алгоритмом \emph{AdaBoost}, дает весьма впечатляющий результаты. Система поиска объектов на изображении \cite{viola01} находит $95\%$ всех искомых объектов с $0,0001\%$ ложных срабатываний.

Какими свойствами должен обладать простой классификатор? В первую очередь, вероятность его ошибки должна быть хотя бы немного меньше $^1/_2$, то есть он должен работать лучше, чем ``подбрасывание монеты'':
\begin{displaymath}
  \exists \gamma > 0: \underset{x \sim D_m}{\Pr}(h(x) \neq y) \leq \frac{1}{2} - \gamma.
\end{displaymath}

Также простой классификатор должен быть максимально простой структуры (обладать малой $VC$-размерностью). Это связано с оценкой ошибки обобщения сильного классификатора.

Самыми часто используемыми на практике простыми классификаторами являются пороги (\emph{stumps}) и \emph{CART} решающие деревья.

\subsubsection{Внутреняя механика AdaBoost}

В этом разделе мы попытаемся пролить немного света на внутреннюю механику алгоритма. Фактически, \emph{AdaBoost} осуществляет два действия:
\begin{itemize}
  \item отбор простых классификаторов (простых признаков);
  \item комбинирование отобранных классификаторов.
\end{itemize}

Первое действие является своеобразным отображением простанства входных векторов в простарнство значений простых классификаторов:
\begin{displaymath}
  x = (x_1, x_2, \dots, x_N) \to (h_1(x), h_2(x), \dots, h_M(x)).
\end{displaymath}

Комбинирование простых классификаторов происходит линейно (составляется линейная комбинация), а решение принимается в зависимости от знака полученной комбинации. Это фактически эквивалентно заделению простанства значений простых классификаторов гиперплоскостью и принятие решения в зависимости от того, по какую сторону гиперплоскости лежит отображение вектора признаков.

Таким образом, готовый классификатор производит вначале отображение в некое простанство, обычно намного более высокой размерности, чем исходное, в котором производит линейную классификацию. На этапе тренировки алгоритм последовательно строит и это отображение, и саму гиперплоскость.

Стоит заметить, что работа \emph{AdaBoost} в значительной мере напоминает работу алгоритма ядерной машины опорных векторов (\emph{Kernel Support Vector Machine}).

Одна из интерпретаций работы алгоритмов на основе \emph{AdaBoost} основана на понятии ``грани'' (\emph{margin}). В случае \emph{AdaBoost} грань определяется как:
\begin{displaymath}
  \mu(x, y) = \frac{\sum_{m = 1}^M{y \cdot f_m(x)}}{\sum_{m = 1}^M{\alpha_m}}.
\end{displaymath}

Эту величину можно интерпретировать как меру ``уверенности'' классификатора в примере $(x, y)$. Если классификация правильная, то грань больше нуля, иначе грань отрицательно. Чем больше простых классификаторов правильно классифицируют пример, тем больше грань.

Если учесть то, как на каждом шаге вычисляются веса примеров, то легко видеть, что на каждом шаге \emph{AdaBoost} пытается максимизировать минимальную грань тренировочной выборки. Утверждается, что данное дейтсвие положительно сказывается на обобщающих способностях алгоритма. Больше про данную интерпретацию семейства алгоритмов на основе \emph{AdaBoost} можно прочитать в \cite{rosset04}.

\subsection{Заключение}
Мы кратко описали самый базовый алгоритм, основывающийся на идеи усиления простых классификаторов. Все последующие его модификации сохраняют основные свойства своего предка. Для более детального знакомства предлагаем обратиться к указанной в библиографии литературе, а также посетить сайт \href{http://www.boosting.org}{http://www.boosting.org}.

\newpage
